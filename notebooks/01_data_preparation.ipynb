{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f858c4a8-bf6d-44b1-b9d6-87c274e1c336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames loaded successfully.\n",
      "\n",
      "✓ First merge completed successfully\n",
      "✓ Second merge completed successfully\n",
      "\n",
      "--- Merged DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 737 entries, 0 to 736\n",
      "Data columns (total 90 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Rk_per100                  736 non-null    object \n",
      " 1   Player                     736 non-null    object \n",
      " 2   Age                        735 non-null    float64\n",
      " 3   Team                       735 non-null    object \n",
      " 4   Pos_per100                 735 non-null    object \n",
      " 5   G_per100                   735 non-null    float64\n",
      " 6   GS_per100                  735 non-null    float64\n",
      " 7   MP_per100                  735 non-null    float64\n",
      " 8   FG                         735 non-null    float64\n",
      " 9   FGA                        735 non-null    float64\n",
      " 10  FG%                        732 non-null    float64\n",
      " 11  3P                         735 non-null    float64\n",
      " 12  3PA                        735 non-null    float64\n",
      " 13  3P%                        691 non-null    float64\n",
      " 14  2P                         735 non-null    float64\n",
      " 15  2PA                        735 non-null    float64\n",
      " 16  2P%                        725 non-null    float64\n",
      " 17  eFG%                       732 non-null    float64\n",
      " 18  FT                         735 non-null    float64\n",
      " 19  FTA                        735 non-null    float64\n",
      " 20  FT%                        694 non-null    float64\n",
      " 21  ORB                        735 non-null    float64\n",
      " 22  DRB                        735 non-null    float64\n",
      " 23  TRB                        735 non-null    float64\n",
      " 24  AST                        735 non-null    float64\n",
      " 25  STL                        735 non-null    float64\n",
      " 26  BLK                        735 non-null    float64\n",
      " 27  TOV                        735 non-null    float64\n",
      " 28  PF                         735 non-null    float64\n",
      " 29  PTS                        735 non-null    float64\n",
      " 30  ORtg                       735 non-null    float64\n",
      " 31  DRtg                       735 non-null    float64\n",
      " 32  Awards_per100              36 non-null     object \n",
      " 33  Player-additional\\_per100  736 non-null    object \n",
      " 34  Rk_adv                     736 non-null    object \n",
      " 35  Pos_adv                    735 non-null    object \n",
      " 36  G_adv                      735 non-null    float64\n",
      " 37  GS_adv                     735 non-null    float64\n",
      " 38  MP_adv                     735 non-null    float64\n",
      " 39  PER                        735 non-null    float64\n",
      " 40  TS%                        732 non-null    float64\n",
      " 41  3PAr                       732 non-null    float64\n",
      " 42  FTr                        732 non-null    float64\n",
      " 43  ORB%                       736 non-null    float64\n",
      " 44  DRB%                       736 non-null    float64\n",
      " 45  TRB%                       736 non-null    float64\n",
      " 46  AST%                       736 non-null    float64\n",
      " 47  STL%                       736 non-null    float64\n",
      " 48  BLK%                       736 non-null    float64\n",
      " 49  TOV%                       733 non-null    float64\n",
      " 50  USG%                       736 non-null    float64\n",
      " 51  OWS                        735 non-null    float64\n",
      " 52  DWS                        735 non-null    float64\n",
      " 53  WS                         735 non-null    float64\n",
      " 54  WS/48                      735 non-null    float64\n",
      " 55  OBPM                       735 non-null    float64\n",
      " 56  DBPM                       735 non-null    float64\n",
      " 57  BPM                        735 non-null    float64\n",
      " 58  VORP                       735 non-null    float64\n",
      " 59  Awards_adv                 36 non-null     object \n",
      " 60  Player-additional\\_adv     736 non-null    object \n",
      " 61  Rk                         736 non-null    object \n",
      " 62  Pos                        735 non-null    object \n",
      " 63  G                          735 non-null    float64\n",
      " 64  GS                         735 non-null    float64\n",
      " 65  MP                         735 non-null    float64\n",
      " 66  FG%_shooting               732 non-null    float64\n",
      " 67  Dist.                      732 non-null    float64\n",
      " 68  %FGA 2P                    732 non-null    float64\n",
      " 69  %FGA 0-3                   732 non-null    float64\n",
      " 70  %FGA 3-10                  732 non-null    float64\n",
      " 71  %FGA 10-16                 732 non-null    float64\n",
      " 72  %FGA 16-3P                 732 non-null    float64\n",
      " 73  %FGA 3P                    732 non-null    float64\n",
      " 74  FG% 2P                     725 non-null    float64\n",
      " 75  FG% 0-3                    706 non-null    float64\n",
      " 76  FG% 3-10                   704 non-null    float64\n",
      " 77  FG% 10-16                  653 non-null    float64\n",
      " 78  FG% 16-3P                  572 non-null    float64\n",
      " 79  FG% 3P                     691 non-null    float64\n",
      " 80  %FG AST 2P                 714 non-null    float64\n",
      " 81  %FG AST 3P                 635 non-null    float64\n",
      " 82  %FGA Dunks                 732 non-null    float64\n",
      " 83  # of Dunks                 735 non-null    float64\n",
      " 84  Coner %3PA                 691 non-null    float64\n",
      " 85  Corner 3P%                 653 non-null    float64\n",
      " 86  Heaves Att.                735 non-null    float64\n",
      " 87  Heaves Md.                 735 non-null    float64\n",
      " 88  Awards                     36 non-null     object \n",
      " 89  -9999\\                     736 non-null    object \n",
      "dtypes: float64(76), object(14)\n",
      "memory usage: 518.3+ KB\n",
      "None\n",
      "\n",
      "Final Merged DataFrame Head (before duplicate handling):\n",
      "  Rk_per100           Player   Age Team Pos_per100  G_per100  GS_per100  \\\n",
      "0         1    Mikal Bridges  28.0  NYK         SF      82.0       82.0   \n",
      "1         2        Josh Hart  29.0  NYK         SG      77.0       77.0   \n",
      "2         3  Anthony Edwards  23.0  MIN         SG      79.0       79.0   \n",
      "3         4     Devin Booker  28.0  PHO         SG      75.0       75.0   \n",
      "4         5     James Harden  35.0  LAC         PG      79.0       79.0   \n",
      "\n",
      "   MP_per100    FG   FGA  ...  %FG AST 2P  %FG AST 3P  %FGA Dunks  # of Dunks  \\\n",
      "0     3036.0   9.7  19.3  ...       0.682       0.957       0.025        28.0   \n",
      "1     2897.0   6.9  13.2  ...       0.613       0.869       0.030        21.0   \n",
      "2     2871.0  12.4  27.7  ...       0.384       0.475       0.039        54.0   \n",
      "3     2795.0  11.6  25.1  ...       0.431       0.716       0.013        14.0   \n",
      "4     2789.0   9.4  22.9  ...       0.176       0.370       0.003         4.0   \n",
      "\n",
      "   Coner %3PA  Corner 3P%  Heaves Att.  Heaves Md.  Awards      -9999\\  \n",
      "0       0.449       0.423         12.0         0.0     NaN  bridgmi01\\  \n",
      "1       0.258       0.308          1.0         0.0     NaN   hartjo01\\  \n",
      "2       0.107       0.414         10.0         0.0  ASNBA2  edwaran01\\  \n",
      "3       0.123       0.338          0.0         0.0     NaN  bookede01\\  \n",
      "4       0.042       0.429          0.0         0.0  ASNBA3  hardeja01\\  \n",
      "\n",
      "[5 rows x 90 columns]\n",
      "\n",
      "--- Handling Duplicate Players (Multi-Team) ---\n",
      "Original final_df shape: (737, 90)\n",
      "Shape after handling duplicates (should be 1 row per player): (571, 90)\n",
      "Final Merged DataFrame Head (after duplicate handling):\n",
      "  Rk_per100           Player   Age Team Pos_per100  G_per100  GS_per100  \\\n",
      "0         1    Mikal Bridges  28.0  NYK         SF      82.0       82.0   \n",
      "1         2        Josh Hart  29.0  NYK         SG      77.0       77.0   \n",
      "2         3  Anthony Edwards  23.0  MIN         SG      79.0       79.0   \n",
      "3         4     Devin Booker  28.0  PHO         SG      75.0       75.0   \n",
      "4         5     James Harden  35.0  LAC         PG      79.0       79.0   \n",
      "\n",
      "   MP_per100    FG   FGA  ...  %FG AST 2P  %FG AST 3P  %FGA Dunks  # of Dunks  \\\n",
      "0     3036.0   9.7  19.3  ...       0.682       0.957       0.025        28.0   \n",
      "1     2897.0   6.9  13.2  ...       0.613       0.869       0.030        21.0   \n",
      "2     2871.0  12.4  27.7  ...       0.384       0.475       0.039        54.0   \n",
      "3     2795.0  11.6  25.1  ...       0.431       0.716       0.013        14.0   \n",
      "4     2789.0   9.4  22.9  ...       0.176       0.370       0.003         4.0   \n",
      "\n",
      "   Coner %3PA  Corner 3P%  Heaves Att.  Heaves Md.  Awards      -9999\\  \n",
      "0       0.449       0.423         12.0         0.0     NaN  bridgmi01\\  \n",
      "1       0.258       0.308          1.0         0.0     NaN   hartjo01\\  \n",
      "2       0.107       0.414         10.0         0.0  ASNBA2  edwaran01\\  \n",
      "3       0.123       0.338          0.0         0.0     NaN  bookede01\\  \n",
      "4       0.042       0.429          0.0         0.0  ASNBA3  hardeja01\\  \n",
      "\n",
      "[5 rows x 90 columns]\n",
      "\n",
      "DataFrame shape after dropping redundant columns: (571, 85)\n",
      "Columns after dropping redundant ones:\n",
      "['Rk_per100', 'Player', 'Age', 'Team', 'Pos_per100', 'G_per100', 'GS_per100', 'MP_per100', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'ORtg', 'DRtg', 'Awards_per100', 'Player-additional\\\\_per100', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP', 'Awards_adv', 'Player-additional\\\\_adv', 'Rk', 'Pos', 'G', 'GS', 'MP', 'FG%_shooting', 'Dist.', '%FGA 2P', '%FGA 0-3', '%FGA 3-10', '%FGA 10-16', '%FGA 16-3P', '%FGA 3P', 'FG% 2P', 'FG% 0-3', 'FG% 3-10', 'FG% 10-16', 'FG% 16-3P', 'FG% 3P', '%FG AST 2P', '%FG AST 3P', '%FGA Dunks', '# of Dunks', 'Coner %3PA', 'Corner 3P%', 'Heaves Att.', 'Heaves Md.', 'Awards', '-9999\\\\']\n",
      "\n",
      "Filtered to players with >= 500 minutes. New shape: (375, 85)\n",
      "\n",
      "--- Missing Value Check (before imputation) ---\n",
      "3P%                9\n",
      "Awards_per100    339\n",
      "Awards_adv       339\n",
      "FG% 10-16          4\n",
      "FG% 16-3P         23\n",
      "FG% 3P             9\n",
      "%FG AST 3P        25\n",
      "Coner %3PA         9\n",
      "Corner 3P%        14\n",
      "Awards           339\n",
      "dtype: int64\n",
      "\n",
      "--- Missing Value Check (after imputation) ---\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Merged and cleaned data (unscaled) saved to ../data/processed/nba_2025_player_stats_merged_cleaned.csv\n",
      "\n",
      "Warning: The following desired features are missing and will be excluded from clustering: ['2P_FG%', '0-3_FG%', '3-10_FG%', '10-16_FG%', '16-3P_FG%']\n",
      "Please check your original CSVs and merge process if these are critical.\n",
      "\n",
      "DataFrame for clustering created with shape: (375, 30)\n",
      "Columns for clustering:\n",
      "['PTS', 'AST', 'TRB', 'STL', 'BLK', 'TOV', 'PF', 'FG%', '3P%', '2P%', 'FT%', 'eFG%', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'BPM', 'VORP', 'Dist.']\n",
      "\n",
      "--- Scaled Features Head ---\n",
      "        PTS       AST       TRB       STL       BLK       TOV        PF  \\\n",
      "0  0.228359 -0.064184 -1.194319 -0.814994 -0.386370 -0.422695 -1.575076   \n",
      "1 -0.587625  0.995455  0.901444  0.571837 -0.633199  0.034899 -0.419198   \n",
      "2  2.239178  0.389947 -0.321084 -0.121579 -0.139541  1.499199 -1.130508   \n",
      "3  1.743759  1.600963 -0.894924 -0.814994 -0.880028  1.133124 -0.330285   \n",
      "4  1.423194  2.622758 -0.221286  0.745191 -0.016126  3.055018 -0.863767   \n",
      "\n",
      "        FG%       3P%       2P%  ...      BLK%      TOV%      USG%       OWS  \\\n",
      "0  0.438961  0.269348  0.795891  ... -0.421257 -0.676091  0.072863  1.032157   \n",
      "1  0.802951  0.064469  1.158172  ... -0.678960  0.871150 -0.686125  1.923393   \n",
      "2 -0.332696  0.669351 -0.607948  ... -0.163555 -0.062530  2.155666  1.503988   \n",
      "3 -0.128862  0.054713  0.010949  ... -0.936662 -0.089206  1.784998  2.290373   \n",
      "4 -0.871400  0.249836 -1.045704  ...  0.029722  1.538064  1.837950  1.189434   \n",
      "\n",
      "        DWS        WS     WS/48       BPM      VORP     Dist.  \n",
      "0  0.510495  0.943915 -0.151930 -0.087366  0.276557  0.128098  \n",
      "1  2.348276  2.282820  0.980710  1.102025  2.017127 -0.589497  \n",
      "2  2.348276  1.976785  0.746991  1.642657  2.742364  0.592424  \n",
      "3 -1.225187  1.211696  0.225617  0.237013  0.639176  0.444684  \n",
      "4  2.858771  1.938531  0.800926  1.642657  2.597317  0.676847  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "Cleaned and scaled data for clustering saved.\n",
      "Player info saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_8/gvzx8lwx3xd08y47f2s1y2rc0000gn/T/ipykernel_35224/1721265537.py:136: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_df[col].fillna(0, inplace=True)\n",
      "/var/folders/_8/gvzx8lwx3xd08y47f2s1y2rc0000gn/T/ipykernel_35224/1721265537.py:141: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_df[col].fillna('Unknown', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define file paths for your CSVs\n",
    "per_100_poss_file = '../data/raw/NBA24-25PER100STATS.csv'\n",
    "advanced_stats_file = '../data/raw/NBA24-25ADVANCEDSTATS.csv'\n",
    "shooting_stats_file = '../data/raw/NBA24-25SHOOTINGSTATS.csv'\n",
    "\n",
    "# Load each CSV into a Pandas DataFrame\n",
    "try:\n",
    "    df_per_100 = pd.read_csv(per_100_poss_file)\n",
    "    df_advanced = pd.read_csv(advanced_stats_file)\n",
    "    df_shooting = pd.read_csv(shooting_stats_file)\n",
    "\n",
    "    print(\"DataFrames loaded successfully.\")\n",
    "    # ... (rest of your initial print statements for head and columns) ...\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: One of the CSV files not found. Please ensure they are in the correct directory.\")\n",
    "    print(f\"Missing file: {e.filename}\")\n",
    "    exit()\n",
    "\n",
    "# ... (your existing checks for required_columns and data type conversions) ...\n",
    "\n",
    "\n",
    "# Now we can merge using the correct column names\n",
    "if all(col in df_per_100.columns for col in ['Player', 'Team', 'Age']) and \\\n",
    "   all(col in df_advanced.columns for col in ['Player', 'Team', 'Age']):\n",
    "\n",
    "    merged_df = pd.merge(df_per_100, df_advanced, on=['Player', 'Team', 'Age'], how='inner', suffixes=('_per100', '_adv'))\n",
    "    print(\"\\n✓ First merge completed successfully\")\n",
    "\n",
    "    if all(col in merged_df.columns for col in ['Player', 'Team', 'Age']) and \\\n",
    "       all(col in df_shooting.columns for col in ['Player', 'Team', 'Age']):\n",
    "\n",
    "        final_df = pd.merge(merged_df, df_shooting, on=['Player', 'Team', 'Age'], how='inner', suffixes=('', '_shooting'))\n",
    "        print(\"✓ Second merge completed successfully\")\n",
    "\n",
    "        print(\"\\n--- Merged DataFrame Info ---\")\n",
    "        print(final_df.info())\n",
    "        print(\"\\nFinal Merged DataFrame Head (before duplicate handling):\")\n",
    "        print(final_df.head())\n",
    "\n",
    "        # --- NEW BLOCK START: Handle Duplicate Players (TOT vs. Individual Teams) ---\n",
    "        print(\"\\n--- Handling Duplicate Players (Multi-Team) ---\")\n",
    "\n",
    "        # Identify players who have 'TOT' in their 'Team' column\n",
    "        tot_players = final_df[final_df['Team'] == 'TOT']\n",
    "\n",
    "        # Get the names of players who have a 'TOT' entry\n",
    "        players_with_tot_entry = tot_players['Player'].unique()\n",
    "\n",
    "        # Create a DataFrame for players who only played for one team (no 'TOT' entry)\n",
    "        # This includes players NOT in the players_with_tot_entry list\n",
    "        # And players who are in the players_with_tot_entry list but are NOT 'TOT' themselves\n",
    "        single_team_players = final_df[\n",
    "            (~final_df['Player'].isin(players_with_tot_entry)) |  # Players who never had a TOT entry\n",
    "            ((final_df['Player'].isin(players_with_tot_entry)) & (final_df['Team'] != 'TOT')) # Players who did, but this isn't their TOT row\n",
    "        ]\n",
    "\n",
    "        # Filter single_team_players to only keep actual single-team players\n",
    "        # For players who had a 'TOT' row, we want to discard their individual team rows.\n",
    "        # So, we'll only take players from `final_df` who are NOT in `players_with_tot_entry`.\n",
    "        # This will correctly keep single-team players only.\n",
    "        final_df_unique_players = final_df[~final_df['Player'].isin(players_with_tot_entry)].copy()\n",
    "\n",
    "        # Now, add back the 'TOT' rows for the multi-team players\n",
    "        final_df_unique_players = pd.concat([final_df_unique_players, tot_players]).copy()\n",
    "\n",
    "        # Sort by Player and ensure no accidental duplicates (e.g., if a TOT row was duplicated during previous merge issues)\n",
    "        # This will give you one row per player (either their single team or their TOT row).\n",
    "        final_df_unique_players.drop_duplicates(subset=['Player', 'Age'], keep='first', inplace=True)\n",
    "\n",
    "\n",
    "        print(f\"Original final_df shape: {final_df.shape}\")\n",
    "        print(f\"Shape after handling duplicates (should be 1 row per player): {final_df_unique_players.shape}\")\n",
    "        # Overwrite final_df with the cleaned version\n",
    "        final_df = final_df_unique_players.copy()\n",
    "        print(\"Final Merged DataFrame Head (after duplicate handling):\")\n",
    "        print(final_df.head())\n",
    "\n",
    "        # --- END NEW BLOCK ---\n",
    "\n",
    "\n",
    "        # --- Initial Data Cleaning and Feature Preparation (rest of your existing blocks) ---\n",
    "\n",
    "        # 1. Handle redundant/duplicate columns from merging\n",
    "        # ... (Your existing code for handling redundant columns) ...\n",
    "        columns_to_drop_after_merge = []\n",
    "        common_redundant_base_cols = ['Rk', 'G', 'GS', 'MP'] # Add more as you inspect final_df.columns if needed\n",
    "\n",
    "        for base_col in common_redundant_base_cols:\n",
    "            if f\"{base_col}_adv\" in final_df.columns and base_col in final_df.columns:\n",
    "                columns_to_drop_after_merge.append(f\"{base_col}_adv\")\n",
    "            if f\"{base_col}_shooting\" in final_df.columns and base_col in final_df.columns:\n",
    "                columns_to_drop_after_merge.append(f\"{base_col}_shooting\")\n",
    "            if f\"{base_col}_adv\" in final_df.columns and f\"{base_col}_per100\" in final_df.columns:\n",
    "                columns_to_drop_after_merge.append(f\"{base_col}_adv\")\n",
    "            if f\"{base_col}_shooting\" in final_df.columns and f\"{base_col}_per100\" in final_df.columns:\n",
    "                columns_to_drop_after_merge.append(f\"{base_col}_shooting\")\n",
    "\n",
    "        if 'Pos_adv' in final_df.columns and 'Pos' in final_df.columns:\n",
    "            columns_to_drop_after_merge.append('Pos_adv')\n",
    "\n",
    "        columns_to_drop_after_merge = list(set(columns_to_drop_after_merge))\n",
    "        final_df.drop(columns=columns_to_drop_after_merge, inplace=True, errors='ignore')\n",
    "        print(f\"\\nDataFrame shape after dropping redundant columns: {final_df.shape}\")\n",
    "        print(\"Columns after dropping redundant ones:\")\n",
    "        print(final_df.columns.tolist())\n",
    "\n",
    "\n",
    "        # 2. Filter out players with insufficient playing time\n",
    "        min_mp_threshold = 500\n",
    "\n",
    "        mp_col_name = 'MP'\n",
    "        if 'MP_per100' in final_df.columns and 'MP' not in final_df.columns: # Adjust based on what remains\n",
    "            mp_col_name = 'MP_per100'\n",
    "\n",
    "        if mp_col_name not in final_df.columns:\n",
    "            print(f\"Error: Could not find a suitable minutes played column ('MP' or 'MP_per100'). Please check your data.\")\n",
    "            exit()\n",
    "\n",
    "        final_df = final_df[final_df[mp_col_name] >= min_mp_threshold].copy()\n",
    "        print(f\"\\nFiltered to players with >= {min_mp_threshold} minutes. New shape: {final_df.shape}\")\n",
    "\n",
    "\n",
    "        # 3. Handling Missing Values (Imputation)\n",
    "        print(\"\\n--- Missing Value Check (before imputation) ---\")\n",
    "        nan_counts = final_df.isnull().sum()\n",
    "        print(nan_counts[nan_counts > 0])\n",
    "\n",
    "        for col in final_df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(final_df[col]):\n",
    "                if final_df[col].isnull().any():\n",
    "                    if any(s in col for s in ['%', 'Ar', 'Dist']) or col in ['PER', 'TS%', 'USG%', 'eFG%', 'FTAr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'BPM', 'VORP']:\n",
    "                        final_df[col].fillna(0, inplace=True)\n",
    "                    else:\n",
    "                        median_val = final_df[col].median()\n",
    "                        final_df[col].fillna(median_val, inplace=True)\n",
    "            elif final_df[col].isnull().any():\n",
    "                final_df[col].fillna('Unknown', inplace=True)\n",
    "\n",
    "        print(\"\\n--- Missing Value Check (after imputation) ---\")\n",
    "        nan_counts_after = final_df.isnull().sum()\n",
    "        print(nan_counts_after[nan_counts_after > 0])\n",
    "\n",
    "\n",
    "        # --- Save the merged and cleaned (unscaled) data ---\n",
    "        output_cleaned_file = '../data/processed/nba_2025_player_stats_merged_cleaned.csv'\n",
    "        final_df.to_csv(output_cleaned_file, index=False)\n",
    "        print(f\"\\nMerged and cleaned data (unscaled) saved to {output_cleaned_file}\")\n",
    "\n",
    "\n",
    "        # 4. Select features for clustering\n",
    "        features_for_clustering = [\n",
    "            'PTS', 'AST', 'TRB', 'STL', 'BLK', 'TOV', 'PF',\n",
    "            'FG%', '3P%', '2P%', 'FT%',\n",
    "            'eFG%', 'TS%',\n",
    "            '3PAr', 'FTr',\n",
    "            'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%',\n",
    "            'OWS', 'DWS', 'WS', 'WS/48', 'BPM', 'VORP',\n",
    "            'Dist.',\n",
    "            '2P_FG%', '0-3_FG%', '3-10_FG%', '10-16_FG%', '16-3P_FG%',\n",
    "        ]\n",
    "\n",
    "        actual_features = [col for col in features_for_clustering if col in final_df.columns]\n",
    "        missing_features = [col for col in features_for_clustering if col not in final_df.columns]\n",
    "\n",
    "        if missing_features:\n",
    "            print(f\"\\nWarning: The following desired features are missing and will be excluded from clustering: {missing_features}\")\n",
    "            print(\"Please check your original CSVs and merge process if these are critical.\")\n",
    "\n",
    "        df_clustering = final_df[actual_features].copy()\n",
    "\n",
    "        # Create player_info DataFrame - Use the correct capitalized names\n",
    "        player_info_cols = ['Player', 'Team', 'Pos', 'Age'] # Corrected to 'Player', 'Team', 'Pos', 'Age'\n",
    "        existing_player_info_cols = [col for col in player_info_cols if col in final_df.columns]\n",
    "        if len(existing_player_info_cols) < len(player_info_cols):\n",
    "            print(f\"Warning: Not all player info columns found. Missing: {list(set(player_info_cols) - set(existing_player_info_cols))}. Player info might be incomplete.\")\n",
    "        player_info = final_df[existing_player_info_cols].copy()\n",
    "\n",
    "\n",
    "        print(f\"\\nDataFrame for clustering created with shape: {df_clustering.shape}\")\n",
    "        print(\"Columns for clustering:\")\n",
    "        print(df_clustering.columns.tolist())\n",
    "\n",
    "\n",
    "        # 5. Feature Scaling\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(df_clustering)\n",
    "        df_scaled = pd.DataFrame(scaled_features, columns=df_clustering.columns)\n",
    "\n",
    "        print(\"\\n--- Scaled Features Head ---\")\n",
    "        print(df_scaled.head())\n",
    "\n",
    "\n",
    "        # 6. Save Processed Data for Clustering (scaled and player info)\n",
    "        df_scaled.to_csv('../data/processed/nba_2025_player_stats_scaled_for_clustering.csv', index=False)\n",
    "        player_info.to_csv('../data/processed/nba_2025_player_info.csv', index=False)\n",
    "        print(\"\\nCleaned and scaled data for clustering saved.\")\n",
    "        print(\"Player info saved.\")\n",
    "\n",
    "    else:\n",
    "        print(\"✗ Cannot perform second merge - missing required columns\")\n",
    "else:\n",
    "    print(\"✗ Cannot perform first merge - missing required columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301e4638-c1c5-4ddc-af7d-6ea6c22493de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered to players with >= 500 minutes. New shape: (375, 82)\n",
      "\n",
      "Warning: The following desired features are missing and will be excluded: ['2P_FG%', '0-3_FG%', '3-10_FG%', '10-16_FG%', '16-3P_FG%', 'Att_3P', 'Att_2P']\n",
      "\n",
      "DataFrame for clustering created with shape: (375, 30)\n",
      "Columns for clustering:\n",
      "['PTS', 'AST', 'TRB', 'STL', 'BLK', 'TOV', 'PF', 'FG%', '3P%', '2P%', 'FT%', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'BPM', 'VORP', 'Dist.']\n"
     ]
    }
   ],
   "source": [
    "# --- Initial Data Cleaning and Feature Selection ---\n",
    "\n",
    "# First, let's make sure we have the merged DataFrame\n",
    "# If final_df is not defined, we need to run the previous cell first\n",
    "try:\n",
    "    final_df\n",
    "except NameError:\n",
    "    print(\"Error: final_df is not defined. Please run the previous cell first to load and merge the data.\")\n",
    "    print(\"Make sure to run the cells in order: 1) Load and merge data, 2) Clean and prepare data\")\n",
    "    exit()\n",
    "\n",
    "# 1. Handle redundant/duplicate columns from merging\n",
    "# Identify columns that are duplicates (e.g., 'MP_per100', 'MP_adv', 'G_per100', 'G_adv', etc.)\n",
    "# It's good practice to inspect `final_df.columns` to see what you have.\n",
    "\n",
    "# Example: Drop redundant minute played columns, keeping one (e.g., from per_100_poss)\n",
    "columns_to_drop_after_merge = []\n",
    "for col in final_df.columns:\n",
    "    if col.endswith('_adv') or col.endswith('_shooting') and col not in ['Player', 'Team', 'Age']: # Updated column names\n",
    "        original_col_name = col.replace('_adv', '').replace('_shooting', '')\n",
    "        if original_col_name in final_df.columns and original_col_name != col: # Check if original (un-suffixed) exists\n",
    "            # We assume the first column (from df_per_100) is the one to keep, drop the others\n",
    "            columns_to_drop_after_merge.append(col)\n",
    "        elif original_col_name + '_per100' in final_df.columns and original_col_name != col:\n",
    "             # If a column like 'MP' from advanced stats is now 'MP_adv' and 'MP_per100' exists, drop 'MP_adv'\n",
    "            columns_to_drop_after_merge.append(col)\n",
    "\n",
    "\n",
    "# Common columns that often exist in multiple tables but you only need one version:\n",
    "common_stats = ['G', 'GS', 'MP'] # Games, Games Started, Minutes Played\n",
    "\n",
    "for stat in common_stats:\n",
    "    if f\"{stat}_adv\" in final_df.columns and f\"{stat}_per100\" in final_df.columns:\n",
    "        columns_to_drop_after_merge.append(f\"{stat}_adv\") # Keep the per100 version\n",
    "    elif f\"{stat}_shooting\" in final_df.columns and f\"{stat}_per100\" in final_df.columns:\n",
    "         columns_to_drop_after_merge.append(f\"{stat}_shooting\") # Keep the per100 version\n",
    "\n",
    "\n",
    "final_df.drop(columns=columns_to_drop_after_merge, inplace=True, errors='ignore') # Use errors='ignore' in case some aren't present\n",
    "\n",
    "\n",
    "# 2. Filter out players with insufficient playing time\n",
    "# This is crucial for meaningful archetypes. For a full season, 500-700 minutes is a good lower bound.\n",
    "# Rookies might have less, so consider your minimum. A common threshold is 15-20 games OR 300-500 minutes.\n",
    "min_mp_threshold = 500 # Example: Minimum 500 minutes played\n",
    "final_df = final_df[final_df['MP'] >= min_mp_threshold].copy() # .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "print(f\"\\nFiltered to players with >= {min_mp_threshold} minutes. New shape: {final_df.shape}\")\n",
    "\n",
    "# 3. Select features for clustering\n",
    "# This is where your basketball knowledge comes in!\n",
    "# Aim for a diverse set of stats that capture different aspects of play.\n",
    "# Avoid highly correlated features initially to prevent redundancy (though PCA can handle this later).\n",
    "\n",
    "# Example feature selection (you'll refine this extensively!)\n",
    "features_for_clustering = [\n",
    "    # Per 100 Possessions (rate stats are generally best for clustering)\n",
    "    'PTS', 'AST', 'TRB', 'STL', 'BLK', 'TOV', 'PF', # Basic volume stats\n",
    "    'FG%', '3P%', '2P%', 'FT%', # Shooting efficiency\n",
    "    # Advanced Stats\n",
    "    'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%',\n",
    "    'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'BPM', 'VORP',\n",
    "    # Shooting Stats (some might be redundant with advanced stats, choose carefully)\n",
    "    'Dist.', # Average Shot Distance\n",
    "    '2P_FG%', '0-3_FG%', '3-10_FG%', '10-16_FG%', '16-3P_FG%', # FG% by distance\n",
    "    'Att_3P', 'Att_2P', # Shot attempts (might already be covered by USG% but can be useful)\n",
    "]\n",
    "\n",
    "# Ensure all selected features exist in your final DataFrame\n",
    "# It's common to find some columns might not exist if they were merged with suffixes\n",
    "# or were only present in one of the original CSVs.\n",
    "# Remove any features from your list that aren't in final_df.columns\n",
    "actual_features = [col for col in features_for_clustering if col in final_df.columns]\n",
    "missing_features = [col for col in features_for_clustering if col not in final_df.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\nWarning: The following desired features are missing and will be excluded: {missing_features}\")\n",
    "\n",
    "df_clustering = final_df[actual_features].copy()\n",
    "\n",
    "# Add Player and Team for later interpretation (but exclude from clustering features)\n",
    "player_info = final_df[['Player', 'Team', 'Pos', 'Age']].copy() # Updated column names\n",
    "\n",
    "print(f\"\\nDataFrame for clustering created with shape: {df_clustering.shape}\")\n",
    "print(\"Columns for clustering:\")\n",
    "print(df_clustering.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284bebbb-a27e-418b-aed3-90350c356555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Missing Value Check (before imputation) ---\n",
      "PTS      0\n",
      "AST      0\n",
      "TRB      0\n",
      "STL      0\n",
      "BLK      0\n",
      "TOV      0\n",
      "PF       0\n",
      "FG%      0\n",
      "3P%      0\n",
      "2P%      0\n",
      "FT%      0\n",
      "PER      0\n",
      "TS%      0\n",
      "3PAr     0\n",
      "FTr      0\n",
      "ORB%     0\n",
      "DRB%     0\n",
      "TRB%     0\n",
      "AST%     0\n",
      "STL%     0\n",
      "BLK%     0\n",
      "TOV%     0\n",
      "USG%     0\n",
      "OWS      0\n",
      "DWS      0\n",
      "WS       0\n",
      "WS/48    0\n",
      "BPM      0\n",
      "VORP     0\n",
      "Dist.    0\n",
      "dtype: int64\n",
      "\n",
      "--- Missing Value Check (after imputation) ---\n",
      "PTS      0\n",
      "AST      0\n",
      "TRB      0\n",
      "STL      0\n",
      "BLK      0\n",
      "TOV      0\n",
      "PF       0\n",
      "FG%      0\n",
      "3P%      0\n",
      "2P%      0\n",
      "FT%      0\n",
      "PER      0\n",
      "TS%      0\n",
      "3PAr     0\n",
      "FTr      0\n",
      "ORB%     0\n",
      "DRB%     0\n",
      "TRB%     0\n",
      "AST%     0\n",
      "STL%     0\n",
      "BLK%     0\n",
      "TOV%     0\n",
      "USG%     0\n",
      "OWS      0\n",
      "DWS      0\n",
      "WS       0\n",
      "WS/48    0\n",
      "BPM      0\n",
      "VORP     0\n",
      "Dist.    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Handle Missing Values ---\n",
    "print(\"\\n--- Missing Value Check (before imputation) ---\")\n",
    "print(df_clustering.isnull().sum())\n",
    "\n",
    "# Strategy: Impute missing numerical values.\n",
    "# For percentages/ratios: often 0 makes sense if it's truly a \"no attempts\" scenario.\n",
    "# For other stats: mean or median imputation can be used.\n",
    "# A robust approach is to check each column individually.\n",
    "\n",
    "for col in df_clustering.columns:\n",
    "    if df_clustering[col].isnull().any():\n",
    "        # A common imputation for stats where NaN means 'no attempts' is 0\n",
    "        if 'FG%' in col or '3P%' in col or 'FT%' in col or 'Ar' in col or 'Dist' in col: # Check for percentage/attempt ratio columns\n",
    "            df_clustering[col].fillna(0, inplace=True)\n",
    "        else:\n",
    "            # For other numerical stats, median is often more robust to outliers than mean\n",
    "            median_val = df_clustering[col].median()\n",
    "            df_clustering[col].fillna(median_val, inplace=True)\n",
    "            print(f\"Imputed missing values in '{col}' with median: {median_val}\")\n",
    "\n",
    "print(\"\\n--- Missing Value Check (after imputation) ---\")\n",
    "print(df_clustering.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f080de3e-a5b0-4649-99b7-4462d3d43fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scaled Features Head ---\n",
      "        PTS       AST       TRB       STL       BLK       TOV        PF  \\\n",
      "0  0.228359 -0.064184 -1.194319 -0.814994 -0.386370 -0.422695 -1.575076   \n",
      "1 -0.587625  0.995455  0.901444  0.571837 -0.633199  0.034899 -0.419198   \n",
      "2  2.239178  0.389947 -0.321084 -0.121579 -0.139541  1.499199 -1.130508   \n",
      "3  1.743759  1.600963 -0.894924 -0.814994 -0.880028  1.133124 -0.330285   \n",
      "4  1.423194  2.622758 -0.221286  0.745191 -0.016126  3.055018 -0.863767   \n",
      "\n",
      "        FG%       3P%       2P%  ...      BLK%      TOV%      USG%       OWS  \\\n",
      "0  0.438961  0.269348  0.795891  ... -0.421257 -0.676091  0.072863  1.032157   \n",
      "1  0.802951  0.064469  1.158172  ... -0.678960  0.871150 -0.686125  1.923393   \n",
      "2 -0.332696  0.669351 -0.607948  ... -0.163555 -0.062530  2.155666  1.503988   \n",
      "3 -0.128862  0.054713  0.010949  ... -0.936662 -0.089206  1.784998  2.290373   \n",
      "4 -0.871400  0.249836 -1.045704  ...  0.029722  1.538064  1.837950  1.189434   \n",
      "\n",
      "        DWS        WS     WS/48       BPM      VORP     Dist.  \n",
      "0  0.510495  0.943915 -0.151930 -0.087366  0.276557  0.128098  \n",
      "1  2.348276  2.282820  0.980710  1.102025  2.017127 -0.589497  \n",
      "2  2.348276  1.976785  0.746991  1.642657  2.742364  0.592424  \n",
      "3 -1.225187  1.211696  0.225617  0.237013  0.639176  0.444684  \n",
      "4  2.858771  1.938531  0.800926  1.642657  2.597317  0.676847  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Feature Scaling ---\n",
    "# StandardScaler (Z-score normalization) is generally preferred for K-Means.\n",
    "# It transforms data to have a mean of 0 and a standard deviation of 1.\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_clustering)\n",
    "\n",
    "# Convert back to a DataFrame for easier handling, keeping column names\n",
    "df_scaled = pd.DataFrame(scaled_features, columns=df_clustering.columns)\n",
    "\n",
    "print(\"\\n--- Scaled Features Head ---\")\n",
    "print(df_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9e4dbf3-7912-400f-9ff3-516298d85f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned and scaled data saved as 'nba_2025_player_stats_scaled_for_clustering.csv'\n",
      "Player info saved as 'nba_2025_player_info.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned and scaled data for clustering\n",
    "df_scaled.to_csv('../data/raw/nba_2025_player_stats_scaled_for_clustering.csv', index=False)\n",
    "player_info.to_csv('../data/raw/nba_2025_player_info.csv', index=False) # Keep player info separate\n",
    "print(\"\\nCleaned and scaled data saved as 'nba_2025_player_stats_scaled_for_clustering.csv'\")\n",
    "print(\"Player info saved as 'nba_2025_player_info.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
